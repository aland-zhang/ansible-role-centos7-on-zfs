---
# Most of the information for this role comes from:
#   https://www.csparks.com/BootFedoraZFS/index.md

- name: 'Centos distro must be version 7.{{ centos_minor_version }}.{{ centos_build_version }}'
  fail:
    msg: 'This is Centos {{ ansible_distribution_version }}'
  when: ansible_distribution == 'CentOS' and ansible_distribution_version != '.'.join(('7', centos_minor_version, centos_build_version))
 
- name: Set SELinux to permissive in current session
  shell: setenforce 0

- name: Set SELinux to permissive after reboot
  lineinfile:
    path: /etc/selinux/config
    regexp: '^SELINUX='
    line: 'SELINUX=permissive'
    
- name: 'Identify the rpm distribution'
  shell: rpm -E %dist
  args:
    warn: no
  register: rpm_dist
  changed_when: false

- name: 'Install epel repo config for CentOS'
  package:
    name: 'epel-release'
  when: ansible_distribution == 'CentOS'

- name: Install additional package dependencies
  package:
    name: libselinux-python, grub2-efi-x64-modules, gdisk
 
- name: Upgrade Distro packages
  package:
    name: "*"
    state: latest

- name: Running and latest installed kernel must match
  shell: |
    set -e   # prevent silent failures
    # find the latest installed kernel image; 
    # strip end of kernel name '{{ rpm_dist.stdout }}.x86_64' to make it possible to compare.
    installedKernel=$( ls -1 /boot/vmlinuz-* | grep x86_64 | sed "s/{{ rpm_dist.stdout }}.x86_64//" | sort | tail -1 | sed "s:/boot/vmlinuz-::" )
    runningKernel=$( uname -r | sed "s/{{ rpm_dist.stdout }}.x86_64//" )
    if [[ "${installedKernel}" != "${runningKernel}" ]]; then
      echo "installedKernel=${installedKernel}"
      echo "  runningKernel=${runningKernel}"
      echo "UPDATED KERNEL REQUIRES REBOOT!"
      exit 1
    fi
  changed_when: false

- name: 'Install zfs repo for {{ target_partition_id }}'
  package:
    name: 'http://download.zfsonlinux.org/fedora/zfs-release{{ rpm_dist.stdout }}.noarch.rpm'
  when: ansible_distribution == 'Fedora'

- name: 'Install zfs repo config for CentOS'
  package:
    name: 'http://download.zfsonlinux.org/epel/zfs-release{{ rpm_dist.stdout }}_{{ centos_minor_version }}.noarch.rpm'
  when: ansible_distribution == 'CentOS'

- name: Kernel is correct version. Install ZFS packages and dependencies.
  package:
    name: zfs, zfs-dracut
 
- name: Is system configured to use EFI?
  shell: rpm -qa | grep grub2-efi
  args:
    warn: no
  register: rpm_list
  failed_when: "rpm_list.stdout_lines|length == 0"
  changed_when: false

- name: Copy zmogrify template onto system
  template:
    src: "{{ role_path }}/templates/zmogrify"
    dest: /usr/local/sbin/
    mode: u=rwx,g=r,o=r

- name: Copy zenter onto system
  copy:
    src: "{{ role_path }}/files/zenter"
    dest: /usr/local/sbin/
    mode: u=rwx,g=r,o=r

- name: 
  lineinfile:
    path: /etc/default/grub
    regexp: '{{ item.name }}'
    line: '{{ item.name }}={{ item.value  }}'
  with_items:
    # Add zfs module to grub:
    # https://www.gnu.org/software/grub/manual/grub/html_node/Simple-configuration.html
    # zmogrify copies modules to: /boot/efi/EFI/{{ ansible_distribution }}/x86_64-efi
    - { name: 'GRUB_PRELOAD_MODULES', value:   '"zfs"' }
    - { name: 'GRUB_DISABLE_OS_PROBER', value: 'true'    }

- name: 'Make zpool status report the full path of devices'
  blockinfile:
    path: /etc/profile.d/grub2_zpool_fix.sh
    create: yes
    block: |
      export ZPOOL_VDEV_NAME_PATH=YES

- name: Kernel upgrades should trigger a zfs driver rebuild 
  file:
    src: "/usr/local/sbin/zmogrify"
    dest: "/etc/kernel/postinst.d/zmogrify"
    state: link

- name: 'Check if zfs module will load during reboot'
  stat:
    path: /etc/modules-load.d/zfs.conf
  register: zfs_module_loading
  no_log: true
  changed_when: false

- name: 'Load zfs module for this session'
  modprobe:
    name: zfs
    state: present
  when: zfs_module_loading.stat.exists == False

- name: 'Ensure zfs module is loaded during boot sequence'
  blockinfile:
    path: /etc/modules-load.d/zfs.conf
    create: yes
    block: |
      # Load zfs kernel module
      zfs

- name: Disable the import cache as it can cause problems as zfs pool changes
  service:
    name: zfs-import-cache
    enabled: no

- name: Enable the import-scan to scan devices 
  service:
    name: zfs-import-scan
    enabled: yes

# - name: Delete unused zfs cache file
#   file:
#     state: absent
#     path: "/etc/zfs/zpool.cache"

- name: rebuild the initramfs excluding the zpool cache
  shell: dracut -fv --kver `uname -r`

- name: Get partition data
  parted: 
    device: '{{ target_device }}'
    unit: 'MiB'
  register: parted_data
  changed_when: false

- name: Check that disk has no partitions before assigning some
  vars:  
    msg_string: |
      # Clean target device 
      sudo zfs umount /sysroot && sudo zpool destroy -f pool && sudo sgdisk -Z {{ target_device }}'
  fail:
    msg: "{{ msg_string.split('\n')[:-1] }}"
  when: parted_data.partitions|length != 0 and second_disk == true
  changed_when: false

- name: 'Create bootable zfs drive on {{ target_device }}'
  shell: |
    set -e   # prevent silent failures
    # Zap the disk
    sgdisk -Z {{ target_device }}
    
    # Create an efi boot partition
    sgdisk -n 1:0:+200M -t 1:ef00 -c 1:"efi_boot"  {{ target_device }}

    # Ensure the boot partition is GPT    
    sgdisk -g {{ boot_efi_partition }}

    # Create a zpool partition on the rest of the disk
    sgdisk -n 2:0:0 -t 2:8300 -c 2:"zfs_pool" {{ target_device }} 

    # inform the OS of partition table changes
    partprobe /dev/sdb
  when: second_disk == true
 
- name: 'Formatting efi boot drive on {{ target_device }}'
  shell: mkfs.fat -F32 {{ boot_efi_partition }}
  when: second_disk == true

- name: Is target partition suitable for zpool
  fail: 
    msg: "Partition not prepped for use by zpool {{ parted_data.partitions[1] }}"
  loop: "{{ parted_data['partitions'] }}"
  when: item.num == target_partition_number and (
        item.fstype != '' or
        item.name != '' )

- name: Wiping zpool meta-data from parition
  shell: |
    set -e   # prevent silent failures
 
    if ! ( zpool list | grep "^pool " ); then
      PARTITION_SIZE=$( sudo blockdev --getsz {{ target_partition }} ) 
      dd if=/dev/zero of={{ target_partition }} bs=512 count=2048
      dd if=/dev/zero of={{ target_partition }} bs=512 count=2048 seek=$((${PARTITION_SIZE} - 2048))
    fi
  when: wipe_partition == 'y'

- name: Create a zpool called pool if not already done
  shell: |
    set -e   # prevent silent failures
 
    if ! ( zpool list | grep "^pool " ); then
      # find the /dev/disk/by-partuuid/${partitionUUID}
      partitionUUID=$( lsblk -ln -o PARTUUID -d {{ target_partition }} )
      # if string empty or link missing
      if [[ -z "${partitionUUID}" ]] || [[ ! -e /dev/disk/by-partuuid/${partitionUUID} ]]; then
        echo "[ERROR] Cannot find the zfs root partuuid!"
        exit 1
      fi
      # Create a zpool called 'pool' on partitionUUID 
      # aligned to sector 2^12 (2048)
      zpool create pool -m none {{ target_partition }} -o ashift=12
      # zpool create pool -m none ${partitionUUID} -o ashift=12
      zfs set atime=off pool

      # Grub failed to boot complaining that kernel needs to be loaded first. 
      # Manually navigating to the kernel image in the grub console 
      #   (grub) ls (hd1,gpt2)/ROOT/{{ target_partition_id }}@/
      #          error: compression algorithm inherit not supported
      # NOTE: This has only been tested on VirtualBox so far
      # Disabled this until problem can be identified
      # Could this be related to GRUB_PRELOAD_MODULES setting?
      # There is a module in 
      ##  zfs set compression=on pool
      #UNTESTED#  zfs inherit compression pool
    fi

    if ! ( zfs list | grep "^pool/ROOT/{{ target_partition_id }} " ) > /dev/null; then
      # Create a dataset on zpool called pool
      zfs create -p pool/ROOT/{{ target_partition_id }}
      # Store extended data directly in inodes
      zfs set xattr=sa pool/ROOT/{{ target_partition_id }}

      # Allows copying OS journal files 
      zfs set acltype=posixacl pool/ROOT/{{ target_partition_id }}
      
      # Allow child datasets to inherit acltype attribute
      # zfs inherit acltype pool/ROOT/{{ target_partition_id }}
    fi
 
    if ! ( zfs list | grep "^pool/ROOT/{{ target_partition_id }}.*/sysroot" ); then
 
      if ( mount | grep "/sysroot" ); then
        umount /sysroot
      fi
 
      # export/import the zpool on an alternative directory
      # This gives flexability of what will be mounted on /
      zpool export pool
      
      zpool import pool -d /dev/disk/by-partuuid -o altroot=/sysroot
    fi
    
    if ! ( mount | grep "^pool/ROOT/{{ target_partition_id }} on /sysroot" ); then
      if ( mount | grep "/sysroot" ); then
        umount /sysroot
      fi
 
      # Mount os dataset onto /sysroot/
      zfs set mountpoint=/ pool/ROOT/{{ target_partition_id }}
      zfs mount pool/ROOT/{{ target_partition_id }}
    fi
  
- name: Copy the installed os from local drive to the zfs mount
  shell: |
    set -e   # prevent silent failures

    rsync --exclude=/proc --exclude=/sys --exclude=/dev  -avxHASX / /sysroot/
    mkdir -p /sysroot/proc
    mkdir -p /sysroot/sys
    mkdir -p /sysroot/dev

- name: Initialise Boot files on efi_boot partition
  shell: | 
    set -e   # prevent silent failures
    # Copy local /boot/efi files onto zfs  boot partition
    cd ~
    mkdir efi_mount
    mount {{ boot_efi_partition }} efi_mount
    cp -a /boot/efi/* efi_mount
    umount efi_mount
    rmdir efi_mount

    # FIXME: Assumes GPT entry is at position 4 (works for me, but...)
    # Remove old GPT entry for build hdd
    efibootmgr -d {{ target_device }} -v -b 4 -B

    # Create a new boot entry in GPT 
    efibootmgr -d {{ target_device }} -v -c -l /EFI/{{ ansible_distribution }}/shimx64.efi -L {{ target_partition_id }}

  when: second_disk == true


- name: Initialise /etc/fstab file to mount new locations
  shell: | 
    set -e   # prevent silent failures

    # Ensure the boot partition auto mounts onto /boot/efi at startup
    # sed: Extract PARTUUID value from blkid output for boot partition
    BLK_PARTUUID=$( blkid {{ boot_efi_partition }} |  sed "s/.* PARTUUID=\"\([[:alnum:]-]\+\)\" .*/\1/" )
    echo "UUID=${BLK_PARTUUID}                     /boot/efi  vfat  umask=0077,shortname=winnt 0 2" >  /sysroot/etc/fstab
    echo "pool/ROOT/{{ target_partition_id }}  /          zfs   default                    0 0" >> /sysroot/etc/fstab

- name: Allow {{ target_partition_id }} to boot into zfs
  shell: |
    set -e   # prevent silent failures
    
    # List all devices for 'pool'; should only be one at this point
    #   Remove lines with '<title>:' at the start and empty lines
    #   Last line should be the device details
    #   Truncate spaces for input to 'cut' 
    # FIXME: Fragile lookup; assumes only one device in pool; probably correct for this scenario
    DISK_ID=$( zpool status -P pool | grep -v "^[ a-z]\+:" | grep -v "^$" | tail -1 | xargs | cut -d' ' -f1 )

    # Check we extracted the right bit.
    if [[ ! -e ${DISK_ID} ]]; then
      echo "Failed to resolve valid device: ${DISK_ID}"
      exit 1
    fi
    echo "GRUB_DEVICE_BOOT=${DISK_ID}" >> /sysroot/etc/default/grub

- name: Setup the /sysroot as zfs boot system
  shell:  zenter /sysroot <<< zmogrify
  register: shell_output
  failed_when: "' error:' in shell_output.stderr"

- name: Check that boot partition has been set correctly
  shell: |
    set -e
    BLK_PARTUUID=$(  blkid {{ boot_efi_partition }} |  sed "s/.* PARTUUID=\"\([[:alnum:]-]\+\)\" .*/\1/" )
    if ! efibootmgr -v -d {{ target_device }} | grep ${BLK_PARTUUID}; then
      echo "Boot disk missing: ${BLK_PARTUUID}"
      exit 1
    fi
    
- name: Check that zfs partition has added to grub
  shell: |
    set -e
    BLK_PARTUUID=$(  blkid {{ target_partition }} |  sed "s/.* PARTUUID=\"\([[:alnum:]-]\+\)\" .*/\1/" )
    if ! grep ${BLK_PARTUUID} /sysroot/boot/efi/EFI/{{ ansible_distribution }}/grub.cfg; then
      echo "ZFS partition is missing: ${BLK_PARTUUID}"
      exit 1
    fi
    
- name: Export the pool for reboot
  shell: |
    # Do some final clean-up
    umount /sysroot
    # Make the pool ready for first import
    zpool export pool

- name: Reboot to ZFS goodness
  debug:
    msg: "All configuration has been done."
  changed_when: false
